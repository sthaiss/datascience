üß† **Gradient Boosting**

**Gradient Boosting √© uma t√©cnica de ensemble supervisionado** que constr√≥i modelos fracos (geralmente √°rvores de decis√£o rasas) de forma sequencial, onde **cada novo modelo corrige os erros do anterior**, minimizando uma **fun√ß√£o de perda via gradiente**.

Usado tanto para **classifica√ß√£o quanto regress√£o**, com vers√µes otimizadas como **XGBoost, LightGBM e CatBoost**.

---

üß© **Como funciona**

1. Come√ßa com uma predi√ß√£o inicial (ex: m√©dia).
2. Calcula o **erro residual** do modelo atual.
3. Ajusta uma **nova √°rvore aos res√≠duos** (gradiente negativo da perda).
4. Soma as √°rvores anteriores, ponderadas por uma taxa de aprendizado.
5. Repete o processo iterativamente.

---

‚öôÔ∏è **Hiperpar√¢metros principais**

* `n_estimators`: n√∫mero de √°rvores (itera√ß√µes).
* `learning_rate`: taxa de aprendizado (quanto corrigir a cada passo).
* `max_depth`: profundidade das √°rvores.
* `subsample`: fra√ß√£o dos dados usada em cada √°rvore (para evitar overfitting).
* `colsample_bytree`: fra√ß√£o de features usada por √°rvore.
* `min_child_weight` (XGBoost) ou `min_data_in_leaf` (LightGBM): m√≠nimo de dados em um n√≥ folha.
* `boosting_type` (LightGBM): `gbdt`, `dart`, etc.

---

üìè **Pr√©-processamento essencial**

* **N√£o lida com valores ausentes diretamente** ‚Üí algumas libs imputam internamente (ex: XGBoost, LGBM).
* **N√£o precisa de normaliza√ß√£o/padroniza√ß√£o**.
* Pode ser sens√≠vel a **outliers e overfitting** se mal parametrizado.
* **Lida bem com classes desbalanceadas**, especialmente com ajuste de `scale_pos_weight` ou `class_weight`.

---

üìä **M√©tricas de avalia√ß√£o**

* Classifica√ß√£o: Acur√°cia, Precis√£o, Recall, F1-score, ROC-AUC.
* Regress√£o: MAE, RMSE, MSE, $R^2$.
* **Valida√ß√£o cruzada** e **early stopping** s√£o cruciais para evitar overfitting.
* **Grid search ou random search** com cross-validation para ajuste fino dos hiperpar√¢metros.

---

‚úÖ **Vantagens**

* **Alta performance** e precis√£o em tarefas complexas.
* Funciona bem com **dados estruturados e tabulares**.
* **Flex√≠vel com diferentes fun√ß√µes de perda**.
* Suporta **paraleliza√ß√£o** (especialmente LightGBM e XGBoost).
* **Boas t√©cnicas internas de regulariza√ß√£o** (shrinkage, subsampling, etc.).

---

‚ö†Ô∏è **Desvantagens**

* **Mais dif√≠cil de interpretar** que modelos simples (√°rvore √∫nica, regress√£o).
* **Custo computacional alto** em datasets grandes.
* **Propenso a overfitting** se n√£o regularizado corretamente.
* N√£o lida com **valores ausentes** nativamente em todas as implementa√ß√µes.
* Pode exigir **bastante tuning** para atingir performance ideal.

---

‚öôÔ∏è **Desempenho computacional**

* Treinamento **mais lento** que modelos simples, mas vers√µes como **LightGBM e XGBoost s√£o otimizadas**.
* **Predi√ß√£o √© relativamente r√°pida**, mesmo com muitas √°rvores.
* **Escala bem com grandes volumes de dados**, especialmente com uso de **GPU** ou paralelismo.

---

üìå **Quando aplicar**

* Quando o objetivo √© **maximizar performance em dados estruturados**.
* Quando se tem **tempo para tuning** e boa capacidade computacional.
* Quando modelos simples n√£o capturam bem a complexidade do problema.
* Ideal para **competi√ß√µes de ML**, **problemas tabulares**, e **produ√ß√£o com alta exig√™ncia de acur√°cia**.

