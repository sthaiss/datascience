‚úÖ O que √©
‚úÖ Como funciona
‚úÖ Quando aplicar
‚úÖ Hiperpar√¢metros
‚úÖ M√©tricas de avalia√ß√£o
‚úÖ Vantagens
‚úÖ Desvantagens

---

üß† **Naive Bayes**

**Naive Bayes √© um algoritmo supervisionado de classifica√ß√£o baseado no Teorema de Bayes**, com a **suposi√ß√£o ing√™nua (naive) de independ√™ncia entre os atributos**. Funciona bem em problemas com **alta dimensionalidade** e **texto categorizado**, como **classifica√ß√£o de e-mails (spam ou n√£o)**.

---

üß© **Como funciona**

Baseia-se na f√≥rmula:

> **P(C|X) ‚àù P(X|C) \* P(C)**

Ou seja, calcula a **probabilidade posterior** de uma classe $C$ dado um conjunto de atributos $X$, assumindo que os atributos s√£o **independentes entre si**.

1. Calcula a **probabilidade de cada classe** com base nos dados de treino.
2. Para um novo exemplo, calcula a **probabilidade de cada classe** dada suas caracter√≠sticas.
3. Retorna a **classe com maior probabilidade**.

---

‚öôÔ∏è **Hiperpar√¢metros principais**

Depende da **varia√ß√£o do Naive Bayes** usada:

* **GaussianNB**: assume distribui√ß√£o normal dos atributos cont√≠nuos.

  * Hiperpar√¢metro: `var_smoothing` (evita divis√£o por zero).
* **MultinomialNB**: para contagens (ex: texto).

  * Hiperpar√¢metro: `alpha` (suaviza√ß√£o de Laplace).
* **BernoulliNB**: para atributos bin√°rios (0/1).

  * Hiperpar√¢metro: `alpha`, `binarize`.

---

üìè **Pr√©-processamento essencial**

* **Imputa√ß√£o de dados ausentes** √© necess√°ria (Naive Bayes padr√£o **n√£o lida diretamente com valores nulos**).
* Multinomial e Bernoulli exigem **atributos positivos**.
* GaussianNB exige aten√ß√£o √† **distribui√ß√£o dos dados cont√≠nuos** (idealmente normal).
* **Outliers** podem afetar o desempenho, especialmente no **GaussianNB**.
* Funciona melhor com **atributos informativos e discretos**.

---

üìä **M√©tricas de avalia√ß√£o**

* **Acur√°cia, Precis√£o, Recall, F1-score, Matriz de Confus√£o**.
* **ROC-AUC** √∫til especialmente com **classes desbalanceadas**.
* **Valida√ß√£o cruzada** √© importante para avaliar a robustez do modelo e evitar overfitting.

---

‚úÖ **Vantagens**

* **Extremamente r√°pido** para treinar e prever.
* Funciona bem com **grandes volumes de dados** e **alta dimensionalidade**.
* **Robusto a atributos irrelevantes** e tolerante a ru√≠do.
* **Poucos hiperpar√¢metros**, f√°cil de ajustar.
* Menor risco de **overfitting**, principalmente com dados simples.

---

‚ö†Ô∏è **Desvantagens**

* Suposi√ß√£o de **independ√™ncia entre atributos muitas vezes √© irrealista**.
* Pode ter **baixa acur√°cia comparado a modelos mais complexos**.
* **Sens√≠vel a outliers** (no caso de GaussianNB).
* N√£o lida com **valores ausentes** diretamente.
* Pode sofrer com **classes desbalanceadas**, favorecendo a classe majorit√°ria.
* Risco de **underfitting** em problemas com intera√ß√µes complexas entre vari√°veis.

---

üìå **Quando aplicar**

* Em **problemas de classifica√ß√£o de texto** (spam, sentimento, t√≥picos).
* Como **modelo baseline r√°pido, simples e interpret√°vel**.
* Em **conjuntos de dados com muitos atributos e poucas amostras**.
* Quando h√° **necessidade de treinar e testar rapidamente**, inclusive com valida√ß√£o cruzada.


