üß† **√Årvore de Decis√£o (Decision Tree)**

**√Årvores de decis√£o s√£o modelos supervisionados que utilizam uma estrutura em forma de √°rvore para tomar decis√µes.** Elas dividem os dados em **n√≥s de decis√£o**, baseando-se em **condi√ß√µes sobre os atributos**, at√© atingir folhas com previs√µes finais (classe ou valor).

Podem ser usadas tanto para **classifica√ß√£o quanto regress√£o**.

---

üß© **Como funciona**

1. O algoritmo escolhe o **atributo que melhor separa os dados**, com base em m√©tricas como:

   * **Gini**, **Entropia** (classifica√ß√£o)
   * **MSE** (regress√£o)
2. Divide recursivamente os dados em **subconjuntos mais puros**.
3. Para prever, segue os ramos da √°rvore com base nas caracter√≠sticas do exemplo.

---

‚öôÔ∏è **Hiperpar√¢metros principais**

* `max_depth`: profundidade m√°xima da √°rvore (limita complexidade).
* `min_samples_split`: n√∫mero m√≠nimo de amostras para dividir um n√≥.
* `min_samples_leaf`: n√∫mero m√≠nimo de amostras em uma folha.
* `criterion`: fun√ß√£o de avalia√ß√£o do split (ex: gini, entropy).
* `max_features`: n√∫mero de atributos considerados por split (ajuda a reduzir overfitting).

---

üìè **Pr√©-processamento essencial**

* **Tolera dados ausentes** (algumas implementa√ß√µes tratam nativamente ou por imputa√ß√£o simples).
* **N√£o exige normaliza√ß√£o ou padroniza√ß√£o** dos dados.
* Pode lidar com **vari√°veis categ√≥ricas e num√©ricas**.
* **Sens√≠vel a outliers**: pode criar splits desbalanceados.
* **Pode se adaptar a dados desbalanceados**, mas a performance pode cair ‚Äî usar pesos ou balanceamento ajuda.

---

üìä **M√©tricas de avalia√ß√£o**

* **Classifica√ß√£o**: Acur√°cia, Precis√£o, Recall, F1-score, Matriz de Confus√£o, ROC-AUC.
* **Regress√£o**: MAE, MSE, RMSE, $R^2$.
* **Valida√ß√£o cruzada** √© essencial para medir a generaliza√ß√£o e evitar overfitting.

---

‚úÖ **Vantagens**

* **F√°cil de entender e interpretar** (visualiza√ß√£o da √°rvore).
* **Pouco pr√©-processamento** necess√°rio.
* Suporta **dados mistos** (categ√≥ricos e num√©ricos).
* Pode lidar com **dados com valores faltantes** em algumas bibliotecas.
* Boa capacidade de **modelar rela√ß√µes n√£o-lineares**.

---

‚ö†Ô∏è **Desvantagens**

* Alta tend√™ncia ao **overfitting**, especialmente com √°rvores profundas.
* **Inst√°vel** a pequenas varia√ß√µes nos dados (gera √°rvores muito diferentes).
* **Sens√≠vel a outliers e ru√≠do**.
* Pode ter **vi√©s para atributos com mais n√≠veis** (categorias).
* **Desempenho pior** que m√©todos ensemble (ex: Random Forest) em datasets complexos.

---

‚öôÔ∏è **Desempenho computacional**

* **Treinamento r√°pido**, especialmente em datasets menores.
* **Predi√ß√£o muito r√°pida** (caminho √∫nico at√© a folha).
* **Escala moderadamente bem** com dados maiores.
* Pode crescer demais se n√£o for controlada (profundidade, m√≠nimo de amostras, etc).

---

üìå **Quando aplicar**

* Quando voc√™ precisa de um modelo **simples, explic√°vel e visualiz√°vel**.
* Em problemas onde a **interpretabilidade √© importante**.
* Como **modelo baseline**, ou como base para m√©todos ensemble como **Random Forest** e **Gradient Boosting**.



