ğŸ§  **RegressÃ£o Linear**

**RegressÃ£o Linear Ã© um modelo supervisionado usado para prever valores contÃ­nuos.** Ela assume uma relaÃ§Ã£o **linear entre os atributos (features) e a variÃ¡vel alvo**, ajustando uma reta (ou hiperplano) que minimiza o erro entre as previsÃµes e os valores reais.

---

ğŸ§© **Como funciona**

1. Aprende os **coeficientes (pesos)** que multiplicam cada variÃ¡vel de entrada.
2. Ajusta a equaÃ§Ã£o da forma:
   $\hat{y} = w_0 + w_1x_1 + w_2x_2 + \ldots + w_nx_n$
3. O objetivo Ã© **minimizar o erro (funÃ§Ã£o de custo)**, normalmente o **Erro QuadrÃ¡tico MÃ©dio (MSE)**.

---

âš™ï¸ **HiperparÃ¢metros principais**

> Obs: a RegressÃ£o Linear "pura" tem poucos hiperparÃ¢metros. Para regularizaÃ§Ã£o, usamos variantes:

* `fit_intercept`: se ajusta o termo independente.
* `normalize`: se normaliza os dados internamente (em versÃµes antigas do sklearn).
* `regularizaÃ§Ã£o`:

  * **Ridge (L2)**: penaliza coeficientes grandes.
  * **Lasso (L1)**: pode zerar coeficientes (seleÃ§Ã£o de variÃ¡veis).
  * **ElasticNet**: combinaÃ§Ã£o de L1 + L2.

---

ğŸ“ **PrÃ©-processamento essencial**

* **SensÃ­vel Ã  escala** â†’ Ã© recomendada a **padronizaÃ§Ã£o dos dados**.
* **NÃ£o lida com valores ausentes** â†’ requer **imputaÃ§Ã£o**.
* **Muito sensÃ­vel a outliers**, que distorcem a linha de regressÃ£o.
* **Multicolinearidade entre variÃ¡veis** pode afetar o modelo â†’ verificar com VIF.
* NÃ£o faz suposiÃ§Ãµes sobre **distribuiÃ§Ã£o dos dados de entrada**, mas **assume resÃ­duo normal e homocedÃ¡stico**.

---

ğŸ“Š **MÃ©tricas de avaliaÃ§Ã£o (regressÃ£o)**

* **MAE (Erro Absoluto MÃ©dio)**
* **MSE (Erro QuadrÃ¡tico MÃ©dio)**
* **RMSE (Raiz do Erro QuadrÃ¡tico MÃ©dio)**
* **$R^2$** (Coeficiente de DeterminaÃ§Ã£o)
* **ValidaÃ§Ã£o cruzada** Ã© importante para avaliar a estabilidade e evitar overfitting em datasets pequenos.

---

âœ… **Vantagens**

* **Simples, rÃ¡pido e interpretÃ¡vel** (coeficientes indicam influÃªncia de cada variÃ¡vel).
* FÃ¡cil de implementar e explicar.
* Requer **pouco poder computacional**.
* Funciona bem quando a **relaÃ§Ã£o Ã© de fato linear**.

---

âš ï¸ **Desvantagens**

* **Assume relaÃ§Ã£o linear**, o que pode ser uma limitaÃ§Ã£o em dados reais.
* **Muito sensÃ­vel a outliers e multicolinearidade**.
* Pode sofrer com **underfitting** em problemas complexos.
* **NÃ£o trata valores ausentes**.
* NÃ£o modela interaÃ§Ãµes ou efeitos nÃ£o lineares sem transformaÃ§Ã£o manual das features.

---

âš™ï¸ **Desempenho computacional**

* **Extremamente rÃ¡pido** para treinar e predizer.
* **Escala bem com grandes volumes de dados**, especialmente se o nÃºmero de features for pequeno.
* Ideal para aplicaÃ§Ãµes com **restriÃ§Ã£o de tempo ou recursos**.

---

ğŸ“Œ **Quando aplicar**

* Quando vocÃª quer um modelo **rÃ¡pido, simples e interpretÃ¡vel**.
* Como **baseline para comparaÃ§Ã£o com modelos mais complexos**.
* Quando os dados mostram **tendÃªncia linear clara** ou **vocÃª precisa entender a contribuiÃ§Ã£o de cada variÃ¡vel**.
