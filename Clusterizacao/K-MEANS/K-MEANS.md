ğŸ§  **K-Means (Clustering)**

**K-Means Ã© um algoritmo nÃ£o supervisionado** que agrupa os dados em **K clusters (grupos)** com base na **similaridade** (geralmente distÃ¢ncia Euclidiana). O objetivo Ã© **minimizar a variÃ¢ncia intra-cluster** (distÃ¢ncia dos pontos ao centroide do grupo).

---

ğŸ§© **Como funciona**

1. Define **K centroides iniciais** (aleatÃ³rios ou por mÃ©todo como K-Means++).
2. Atribui cada ponto ao **cluster mais prÃ³ximo** com base na distÃ¢ncia.
3. Recalcula os centroides como **mÃ©dia dos pontos de cada cluster**.
4. Repete os passos 2 e 3 atÃ© convergir (sem mudanÃ§as relevantes nos grupos).

---

âš™ï¸ **HiperparÃ¢metros principais**

* `n_clusters (K)`: nÃºmero de clusters desejado.
* `init`: mÃ©todo de inicializaÃ§Ã£o (`random`, `k-means++`).
* `max_iter`: nÃºmero mÃ¡ximo de iteraÃ§Ãµes.
* `n_init`: quantas vezes o algoritmo serÃ¡ rodado com diferentes inicializaÃ§Ãµes.
* `tol`: tolerÃ¢ncia para convergÃªncia.

---

ğŸ“ **PrÃ©-processamento essencial**

* **Altamente sensÃ­vel Ã  escala dos dados** â†’ **padronizar ou normalizar** Ã© fundamental.
* **NÃ£o lida com valores ausentes** â†’ imputaÃ§Ã£o necessÃ¡ria.
* **SensÃ­vel a outliers**, que podem puxar centroides.
* K-Means assume **variÃ¢ncia esfÃ©rica** (dados em clusters de forma circular/similar tamanho).

---

ğŸ“Š **MÃ©tricas de avaliaÃ§Ã£o (clustering)**

* **InÃ©rcia** (soma das distÃ¢ncias intra-cluster) â€“ usada internamente.
* **Silhouette Score** â€“ avalia separaÃ§Ã£o e coesÃ£o dos clusters.
* **Davies-Bouldin Index**, **Calinski-Harabasz Score**.
* **ValidaÃ§Ã£o cruzada nÃ£o se aplica diretamente**, mas tÃ©cnicas como **elbow method** ou **gap statistic** ajudam a definir K ideal.

---

âœ… **Vantagens**

* **Simples, rÃ¡pido e fÃ¡cil de interpretar**.
* **EscalÃ¡vel para grandes volumes de dados**.
* Funciona bem quando os clusters tÃªm **forma aproximadamente esfÃ©rica**.
* RazoÃ¡vel para **segmentaÃ§Ã£o inicial** ou **exploraÃ§Ã£o de padrÃµes**.

---

âš ï¸ **Desvantagens**

* Requer definiÃ§Ã£o de **K (nÃºmero de clusters)** antecipadamente.
* **SensÃ­vel a outliers e inicializaÃ§Ã£o aleatÃ³ria**.
* SupÃµe clusters de **tamanho e forma semelhantes** (pode falhar com formatos complexos).
* **NÃ£o garante soluÃ§Ã£o Ã³tima global** (soluÃ§Ã£o pode depender da inicializaÃ§Ã£o).
* **NÃ£o lida com valores ausentes** diretamente.

---

âš™ï¸ **Desempenho computacional**

* **Muito eficiente** para datasets grandes e de baixa/mÃ©dia dimensionalidade.
* Treinamento Ã© rÃ¡pido, mas pode demorar se `K` ou `n_init` forem grandes.
* Escala bem com paralelizaÃ§Ã£o e versÃµes otimizadas (ex: MiniBatchKMeans).

---

ğŸ“Œ **Quando aplicar**

* Quando vocÃª precisa **segmentar dados sem rÃ³tulos**.
* Em anÃ¡lises exploratÃ³rias, como **segmentaÃ§Ã£o de clientes**, **agrupamento de produtos**, etc.
* Quando hÃ¡ **suspeita de padrÃµes naturais nos dados** e Ã© necessÃ¡rio descobri-los.
* Quando os dados sÃ£o **padronizados** e vocÃª pode assumir clusters com forma esfÃ©rica.

